\documentclass[a4paper, 11pt, notitlepage, english]{article}

\usepackage{babel}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc, url}
\usepackage{textcomp}
\usepackage{amsmath, amssymb}
\usepackage{amsbsy, amsfonts}
\usepackage{graphicx, color}
\usepackage{parskip}
\usepackage{framed}
\usepackage{amsmath}
\usepackage{xcolor}
\usepackage{multicol}
\usepackage{url}
\usepackage{flafter}


\usepackage{geometry}
\geometry{headheight=0.01mm}
\geometry{top=24mm, bottom=29mm, left=39mm, right=39mm}

\renewcommand{\arraystretch}{2}
\setlength{\tabcolsep}{10pt}
\makeatletter
\renewcommand*\env@matrix[1][*\c@MaxMatrixCols c]{%
  \hskip -\arraycolsep
  \let\@ifnextchar\new@ifnextchar
  \array{#1}}
%
% Parametere for inkludering av kode fra fil
%
\usepackage{listings}
\lstset{language=python}
\lstset{basicstyle=\ttfamily\small}
\lstset{frame=single}
\lstset{keywordstyle=\color{red}\bfseries}
\lstset{commentstyle=\itshape\color{blue}}
\lstset{showspaces=false}
\lstset{showstringspaces=false}
\lstset{showtabs=false}
\lstset{breaklines}

%
% Definering av egne kommandoer og miljøer
%
\newcommand{\dd}[1]{\ \text{d}#1}
\newcommand{\f}[2]{\frac{#1}{#2}} 
\newcommand{\beq}{\begin{equation}}
\newcommand{\eeq}{\end{equation}}
\newcommand{\bra}[1]{\langle #1|}
\newcommand{\ket}[1]{|#1 \rangle}
\newcommand{\braket}[2]{\langle #1 | #2 \rangle}
\newcommand{\braup}[1]{\langle #1 \left|\uparrow\rangle\right.}
\newcommand{\bradown}[1]{\langle #1 \left|\downarrow\rangle\right.}
\newcommand{\av}[1]{\left| #1 \right|}
\newcommand{\op}[1]{\hat{#1}}
\newcommand{\braopket}[3]{\langle #1 | {#2} | #3 \rangle}
\newcommand{\ketbra}[2]{\ket{#1}\bra{#2}}
\newcommand{\pp}[1]{\frac{\partial}{\partial #1}}
\newcommand{\ppn}[1]{\frac{\partial^2}{\partial #1^2}}
\newcommand{\up}{\left|\uparrow\rangle\right.}
\newcommand{\upup}{\left|\uparrow\uparrow\rangle\right.}
\newcommand{\down}{\left|\downarrow\rangle\right.}
\newcommand{\downdown}{\left|\downarrow\downarrow\rangle\right.}
\newcommand{\updown}{\left|\uparrow\downarrow\rangle\right.}
\newcommand{\downup}{\left|\downarrow\uparrow\rangle\right.}
\newcommand{\bupup}{\left.\langle\uparrow\uparrow\right|}
\newcommand{\bdowndown}{\left.\langle\downarrow\downarrow\right|}
\newcommand{\bupdown}{\left.\langle\uparrow\downarrow\right|}
\newcommand{\bdownup}{\left.\langle\downarrow\uparrow\right|}
\renewcommand{\d}{{\rm d}}
\renewcommand{\b}{\bigg}
\newcommand{\Res}[2]{{\rm Res}(#1;#2)}
\newcommand{\To}{\quad\Rightarrow\quad}
\newcommand{\eps}{\epsilon}
\newcommand{\inner}[2]{\langle #1 , #2 \rangle}


\newcommand{\bt}[1]{\boldsymbol{#1}}
\newcommand{\mat}[1]{\textsf{\textbf{#1}}}
\newcommand{\I}{\boldsymbol{\mathcal{I}}}
\newcommand{\p}{\partial}
%
% Navn og tittel
%
\author{}
\title{Notes for project 1 in FYS4460}


\begin{document}

Introduce the Noose-Hover thermostat and measure rms-displacement using various
thermostats. Discuss advantages and disadvantages of the various thermostats
for various thermodynamic measures.

\section*{Fluctuations}

When measuring a constant temperature system, the temperature should fluctuate. In fact, this can easily be shown from the Boltzmann distribution. As we know the distribution of velocities in the gas, we know the first and second instantaneous kinetic energy of the system and can easily show that
$$\frac{\sigma_{T_k}^2}{\langle T_k\rangle^2_{NVT}} = \frac{2}{3N}.$$
So in any finite system, the relative IKT should fluctuate. In the thermodynamic limit $N\to\infty$ and the relative size of the fluctuations go to zero.

If we use a naive velocity-scaling algorithm, we allow no such temperature fluctuations, thusly, we do not sample the cannonical ensemble, but rather an isokinetic system. 

Frenkel \& Smit state that 
\begin{itemize}
	\item The ad hoc methods yield only the desired kinetic energy per perticle, but otherwise do not correspond to any known ensemble. Of course, any kind of temperature regulation, no matter how unphysical, can be used while preparing the system at a desired temperature (i.e., during equilibration). But, as efficient MD schemes exist that do generate a true canonical distribution, there is little need to use more suspect techniques to fix the temperature.
\end{itemize}

\section*{Andersson Thermostat}

The Andersson thermostat can be considered a Monte Carlo method. It simulates connection with a heat bath stochastically. The stochastic nature of the collision ensures that all accesible constant-energy shells are visited according to their Boltzmann weight.

The Andersson thermostat can be shown to sample the cannonical ensemble, this is explained in detail in Frenkel \& Smit. It is a good thermostat to obtain time-independant properties, such as the equation-of-state. However, it cannot be used to obtain dynamic (time-dependent) properties, such as the diffusion constant. This is due to unphysical dynamics in the system. The random collisions leads to random decorrelation of particles velocity, meaning a particle that would move far in a given a direction may suddenly switch direction, leading to a far smaller mean square displacement.

Any property measured should be independent of the stochastic collision frequency over long time scales. The coupling-strength to the heat bath should not disturb measurements.

\section*{Conclusion}

Andersen thermostat correctly samples the cannonical phase space, but destroys the trajectory. It should therefore not be used to measure any quantities that depend on the trajectories, i.e., any quantities that are time-dependant. We can still use it to estimate equilibrium values for our bulk material, since that only depends on the possible configurations, and the andersen thermostat correctly samples this phase space. We saw this very clearly in the measurement of the diffusion constant, which was way to low for the Andersen thermostat (by a factor of 4!) since the dynamics are broken by the random collisions.


\section*{Thermostats}

The objective for a thermostat is to up- or downregulate the temperature of a given MD system. Reasons we might want to use a thermostat include
\begin{itemize}
	\item Initialize a system at a specfici temperature
	\item Sample a cannonical ensamble (NVT) instead of a micro-cannonical ensamble (NVE)
\end{itemize}

A thermostat effectively simulates the connection of the system to a heat bath. For a thermostat to be labeled \emph{effective} or \emph{good} it is not only sufficient to get and or keep the system at a given temperature, but it should also
\begin{itemize}
	\item Preserve real thermodynamics, i.e., it should sample the correct distribution for the cannonical ensamble---the Boltzmann distribution.
	\item Preserve real dynamics, this is a bit harder to define, as it depends on what kind of measurement or result we are interested in. As we will see in this project for example. A bad theromstat might very well keep the system at the right temperature, but due to the nature of the temperature-regulation, the diffusion of the system is disturbed from what it should be in a real physical system.
\end{itemize}
If a thermostat fails to do this, it might lead to artifacts that are non-physical, i.e., that don't show up in the real world. Or they might give us wrong statistics, meaning our result will not agree with statistics. A good example is the flying ice cube effect, which we will come back to.

There are many possible thermostats that are commonly used, some examples are
\begin{itemize}
	\item Berendsen thermostat
	\item Andersen thermostat
	\item Nóse-Hoover
\end{itemize}



\section*{Thermostats}

The most straight-forward and `naive' thermostat is normal velocity scaling. We know that the macroscopic temperature is measured from the kinetic energy
$$\frac{3}{2}N k_b T = \langle E_k \rangle.$$
If the temperature of our system is above or below our target temperature, that means the average kinetic energy of our particles is too low or to high respectively. We can thus manipulate the temperature of our system by scaling all the velocities in our system to reach the target temperature. If the temperature of our system is given as $T(t)$ and we scale all the velocities in our system by a factor of $\lambda$, we get a change in temperature of
$$\Delta T = T(t+\Delta t) - T(t) = (\lambda^2 - 1)T(t).$$
Solving for the scaling factor gives
$$\lambda = \sqrt{T_0/T(t)}.$$
Velocity scaling isn't very physical, for one it does not allow fluctations of the temperature, which are normally present in the cannonical ensamble.

\subsection*{Berendsen temperature coupling}

Instead of scaling the velocity straight to the target temperature, we could do it gradually. This is the basic idea of the Berendsen temperature coupling thermostat. We now let the gradient of the temperature change be proportional to the temperature difference between the system and the reservoir, just like Fouriers law predicts. 
$$\frac{\d T}{\d t} = \frac{1}{\tau}(T(t) - T_0).$$
the scaling parameter $\tau$ defines how strongly coupled the system is to the reservoir. This gives an exponential decay in the temperature difference between the reservoir and system, a timestep will give a temperature change
$$\Delta T = \frac{\Delta t}{\tau}(T_0 - T(t)).$$
And following the same scaling formula, this gives
$$\frac{\Delta t}{\tau}(T_0 - T(t)) = (\gamma^2 - 1)T(t),$$
solving for $\gamma$ now results in
$$\gamma = \sqrt{1 + \frac{\Delta t}{\tau}\b(\frac{T_0}{T} - 1\b)}.$$
The coupling constant $\tau$ has to be chosen carefully, in the limit $\tau \to \infty$ the thermostat is completely inactive, and we have a microcannonical ensamble. If $\tau$ is to small however, we approach the simple velocity scaling, meaning the temperature fluctations are to small ($\tau = \Delta t$ reproduces simple velocity scaling). In our projects we used values of $\tau \approx 20 \Delta t$, which agrees with the quoted value of $\tau \approx 0.1$ ps

\subsection*{Use of several thermostats}

One may use several thermostats in the same simulations, either for equilibriation and sampling phases. Or simultaneously for various substances to avoid the flying ice cube effect.

\clearpage

\section*{Microcannonical Ensamble}

The microcannonical ensamble has constant energy, but the temperature will fluctate. We start our program with the atoms in a perfect FCC lattice, and stochastic starting velocities drawn from some form of given distribution (Maxwell, Boltzmann, uniform, etc). The system obviously does not start of in thermal equilibrium, and so needs time to equilibriate before we can do statistical sampling.

\section*{Statistical sampling}

The fundamental quantities in our simulation are the position and velocities of all the particles. From these we can easily find the kinetic energies of all the particles and the potential energy of all pair of atoms.

To find the temperature and pressure of the system however, requires us to invoke various statistical mechanics theorems, like the equipartition theorem and the virial theorem.

The equipartition theorem states that any quadratic degree of freedom in a system carries on average an energy of $\frac{1}{2}k_b T$ when the system is in thermal equilibrium. If we sum over all the quadratic degrees of freedom in our system, and there are three for every particle, we then get the total thermal energy $$\frac{3}{2}Nk_b T,$$ and this total thermal energy must be equal to the kinetic energy of the system, so we have 
$$ \frac{3}{2}Nk_b T = \langle E_k \rangle.$$

The virial theorem states
$$P = \rho k_b T - \frac{1}{3}\langle \sum_{i<j} \vec{F}_{ij}\cdot \vec{r}_{ij}.$$
Where $\rho = N/V$ is the particle density. Using the virial theorem, we can find the average pressure of the system from the positions of the atoms solely (the forces are given as functions of the positions).


\section*{Units}

We will measure distance in units of $\sigma$, so
$$\bar{r} = \frac{r}{\sigma}.$$

We measure energy in units of $\eps$, so we have
$$\bar{U} = \frac{U}{\eps}.$$

The force is given as the derivative of the potential
$$F = -\frac{\d U}{\d r},$$
substituting $\bar U$ and $\bar r$ gives
$$F = \bigg(\frac{\eps}{\sigma}\bigg)\bigg(-\frac{\d \bar{U}}{\d \bar{r}}\bigg) = \bigg(\frac{\eps}{\sigma}\bigg)\bar{F},$$
so we see that
$$\bar{F} = \bigg(\frac{\sigma}{\eps}\bigg)F.$$

We want Newton's 2.\ law to simplify to
$$\bar{a} = \bar{F},$$
let us see what this means. From the definition of acceleration we get
$$a = \frac{\d^2 r}{\d t^2} = \frac{\sigma}{t_0^2} \frac{\d^2 \bar{r}}{d\bar{t}^2} = \frac{\sigma}{t_0^2}\bar{a}.$$ 
So we have
$$a = \frac{F}{m} \qquad \Rightarrow \qquad  \frac{\sigma}{t_0^2}\bar{a} = \bigg(\frac{\eps}{\sigma}\bigg)\frac{\bar{F}}{m} \qquad \Rightarrow \qquad \frac{\sigma}{t_0^2} = \bigg(\frac{\eps}{m\sigma}\bigg).$$
Solving for $t$ gives
$$t_0 = \sigma\sqrt{\frac{m}{\eps}}.$$
Which will be our unit of time.

This means we find the kinetic energy from
$$K = \frac{1}{2}mv^2 = \frac{1}{2}\frac{\sigma}{mt_0^2}\bar{v}^2 = \frac{1}{2}\eps \bar{v}^2 \quad \Rightarrow \quad \bar{K} = \frac{1}{2}\bar{V}^2.$$

And we can find the temperature from for example
$$\frac{1}{2}mv_{\rm rms}^2 = \frac{3}{2}k_b T \quad \To \quad \frac{1}{2}\bar{v}_{\rm rms} = \frac{3}{2}\frac{k_b}{\eps} T \quad \To \quad \frac{1}{2}\bar{v}_{\rm rms} = \frac{3}{2}\bar {T},$$
giving us
$$\bar{T} = \frac{k_b}{\eps}T.$$

\section*{Nosé-Hoover}

The equations of motions are as follows
\begin{align*}
\dot{r}_i &= v_i, \\
\dot{v}_i &= \frac{F_i}{m} - \gamma v_i, \\
\dot{\gamma} &= \frac{1}{Q}\bigg(\sum_{i} \frac{p_i^2}{m_i} - \frac{L}{\beta}\bigg).
\end{align*}

Fails for certain systems, due to conservation laws. Frenkel \& Smit shows this for a Harmonic Oscillator. The Anderssen thermostat does not have the same problem. This can be solved using Nosé-Hooved chains, where the additional heat baths compensate for the additional conservated properties.

Much harder to implement, as it is no longer integrator-independant. We now have to also integrate the friciton variable $\gamma$. Through $\gamma$, the acceleration now explicitly depends on the velocity, and so we need to use a predictor-corrector scheme or iteratively solve them.

\clearpage

Possible plots
\begin{itemize}
	\item Show that the maxwell-distribution is achieved for all collision frequencies $\nu$.
	\item Show that the diffusion coefficient is lowered as the collision frequency increases.
\end{itemize}

\clearpage




\end{document}



